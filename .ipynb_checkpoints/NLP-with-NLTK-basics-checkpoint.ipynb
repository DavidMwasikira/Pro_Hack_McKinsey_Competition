{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Hackerthon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving The Hackathon\n",
    "Let’s break the solution into 6 parts as given below for better understanding.\n",
    "\n",
    " 1. Exploratory Data Analysis: A Simple analysis of Data \n",
    " 2. Data cleaning \n",
    " 3. Data preprocessing: Count Vectors and TF-IDF Vectors\n",
    " 4. Training the classifier\n",
    " 5. Predicting for the test set\n",
    " 6. Submitting your solution at MachineHack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: A Simple analysis of Data\n",
    "Let’s start off with the usual drill and import all the necessary modules for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#Download the following modules once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s do a simple analysis of the data in hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the training set\n",
    "train_data = pd.read_excel(\"Datasets/Data_Train.xlsx\")\n",
    "\n",
    "#Printing the top 5 rows\n",
    "print(train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the dataset info\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the shape of the dataset\n",
    "print(train_data.shape)]\n",
    "\n",
    "Out:(7628, 2)\n",
    "\n",
    "#Printing the group by description of each category\n",
    "train_data.groupby(\"SECTION\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates to avoid overfitting\n",
    "train_data.drop_duplicates(inplace = True)\n",
    "\n",
    "#A punctuations string for reference (added other valid characters from the dataset)\n",
    "all_punctuations = string.punctuation + '‘’,:”][],' \n",
    "\n",
    "#Method to remove punctuation marks from the data\n",
    "def punc_remover(raw_text):\n",
    "no_punct = \"\".join([i for i in raw_text if i not in all_punctuations])\n",
    "return no_punct\n",
    "\n",
    "#Method to remove stopwords from the data\n",
    "def stopword_remover(no_punc_text):\n",
    "words = no_punc_text.split()\n",
    "no_stp_words = \" \".join([i for i in words if i not in stopwords.words('english')])\n",
    "return no_stp_words\n",
    "\n",
    "#Method to lemmatize the words in the data\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def lem(words):\n",
    "return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])\n",
    "\n",
    "#Method to perform a complete cleaning\n",
    "def text_cleaner(raw):\n",
    "cleaned_text = stopword_remover(punc_remover(raw))\n",
    "return lem(cleaned_text)\n",
    "\n",
    "#Testing the cleaner method\n",
    "text_cleaner(\"Hi!, this is a sample text to test the text cleaner method. Removes *@!#special characters%$^* and stopwords. And lemmatizes, go, going - run, ran, running\")\n",
    "\n",
    "Out: 'Hi sample text test text cleaner method Removes special character stopwords And lemmatizes go go run run run'\n",
    "\n",
    "#Applying the cleaner method to the entire data\n",
    "train_data['CLEAN_STORY'] = train_data['STORY'].apply(text_cleaner)\n",
    "\n",
    "#Checking the new dataset\n",
    "print(train_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Count Vectors and TF-IDF Vectors\n",
    "Creating Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing sklearn’s Countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Creating a bag-of-words dictionary of words from the data\n",
    "bow_dictionary = CountVectorizer().fit(train_data['CLEAN_STORY'])\n",
    "\n",
    "#Total number of words in the bow_dictionary\n",
    "len(bow_dictionary.vocabulary_)\n",
    "\n",
    "Out : 35189\n",
    "\n",
    "#Using the bow_dictionary to create count vectors for the cleaned data.\n",
    "bow = bow_dictionary.transform(train_data['CLEAN_STORY'])\n",
    "\n",
    "#Printing the shape of the bag of words model\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing TfidfTransformer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Fitting the bag of words data to the TF-IDF transformer\n",
    "tfidf_transformer = TfidfTransformer().fit(bow)\n",
    "\n",
    "#Transforming the bag of words model to TF-IDF vectors\n",
    "storytfidf = tfidf_transformer.transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Multinomial Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Fitting the training data to the classifier\n",
    "classifier = MultinomialNB().fit(storytfidf, train_data['SECTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting For The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and cleaning the test data\n",
    "test_data = pd.read_excel(\"Datasets/Data_Test.xlsx\")\n",
    "test_data['CLEAN_STORY'] = test_data['STORY'].apply(text_cleaner)\n",
    "\n",
    "#Printing the cleaned data\n",
    "print(test_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Pipeline To Pre-Process The Data & Initialise The Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Pipeline module from sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Initializing the pipeline with necessary transformations and the required classifier\n",
    "pipe = Pipeline([\n",
    "('bow', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('classifier', MultinomialNB())])\n",
    "\n",
    "#Fitting the training data to the pipeline\n",
    "pipe.fit(train_data['CLEAN_STORY'], train_data['SECTION'])\n",
    "\n",
    "#Predicting the SECTION\n",
    "test_preds_mnb = pipe.predict(test_data['CLEAN_STORY'])\n",
    "\n",
    "#Writing the predictions to an excel sheet\n",
    "pd.DataFrame(test_preds_mnb, columns = ['SECTION']).to_excel(\"Predictions/predictions.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Your Solution At MachineHack\n",
    "Finally, head to MachineHack, and submit your excel fine at the Submission Deck of the hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
