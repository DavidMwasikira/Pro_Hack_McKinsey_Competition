{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Grid Search CV in sklearn, Keras, XGBoost, LightGBM in Python\n",
    "GridSearchCV is a brute force on finding the best hyperparameters for a specific dataset and model. Why not automate it to the extend we can?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Prepared Dataset\n",
    "\n",
    "MNIST, Boston House Prices and Breast Cancer.\n",
    "\n",
    "## MNIST dataset\n",
    "\n",
    "For the MNIST dataset, we normalize the pictures, divide by the RGB code values and one-hot encode our output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# PREPROCESSING\n",
    "def preprocess_mnist(x_train, y_train, x_test, y_test):\n",
    "    # Normalizing all images of 28x28 pixels\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "    # Float values for division\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Normalizing the RGB codes by dividing it to the max RGB value\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    # Categorical y values\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test= to_categorical(y_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, input_shape\n",
    "    \n",
    "X_train, y_train, X_test, y_test, input_shape = preprocess_mnist(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston House Prices Dataset\n",
    "\n",
    "For the house prices dataset, we do even less preprocessing. We really just remove a few columns with missing values, remove the rest of the rows with missing values and one-hot encode the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "\n",
    "For the last dataset, breast cancer, we don't do any preprocessing except for splitting the training and testing dataset into train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    if do_probabilities:\n",
    "      pred = fitted_model.predict_proba(X_test_data)\n",
    "    else:\n",
    "      pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "## 1. Keras\n",
    "Firtly, we define the neural network architecture, and since it's for the MNIST dataset that consists of pictures, we define it as some sort of convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readying neural network model\n",
    "def build_cnn(activation = 'relu',\n",
    "              dropout_rate = 0.2,\n",
    "              optimizer = 'Adam'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "              activation=activation,\n",
    "              input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=optimizer, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just define the parameters and model to input into the algorithm_pipeline; we run classification on this dataset, since we are trying to predict which class a given image can be categorized into. Note that I commented out some of the parameters, because it would take a long time to train, but you can always fiddle around with which parameters you want. You could even add pool_size or kernel_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              'epochs':[1,2,3],\n",
    "              'batch_size':[128]\n",
    "              #'epochs' :              [100,150,200],\n",
    "              #'batch_size' :          [32, 128],\n",
    "              #'optimizer' :           ['Adam', 'Nadam'],\n",
    "              #'dropout_rate' :        [0.2, 0.3],\n",
    "              #'activation' :          ['relu', 'elu']\n",
    "             }\n",
    "\n",
    "model = KerasClassifier(build_fn = build_cnn, verbose=0)\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                        param_grid, cv=5, scoring_fit='neg_log_loss')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this GridSearchCV, we get the best score and best parameters to be:\n",
    "\n",
    ">-0.04399333562212302\n",
    "{'batch_size': 128, 'epochs': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing bug for scoring with Keras\n",
    "I came across this issue when coding a solution trying to use accuracy for a Keras model in GridSearchCV â€“ you might wonder why 'neg_log_loss' was used as the scoring method?\n",
    "\n",
    "The solution to using something else than negative log loss is to remove some of the preprocessing of the MNIST dataset; that is, REMOVE the part where we make the output variables categorical\n",
    "\n",
    "#### Categorical y values\n",
    "- y_train = to_categorical(y_train)\n",
    "- y_test= to_categorical(y_test)\n",
    "\n",
    "Surely we would be able to run with other scoring methods, right? Yes, that was actually the case (see the notebook). This was the best score and best parameters:\n",
    "\n",
    "> 0.9858\n",
    "{'batch_size': 128, 'epochs': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost\n",
    "Next we define parameters for the boston house price dataset. Here the task is regression, which I chose to use XGBoost for. I also chose to evaluate by a Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'subsample': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score and parameters for the house prices dataset found from the GridSearchCV was\n",
    "\n",
    ">3.4849014783892733\n",
    "{'colsample_bytree': 0.8, 'max_depth': 20, 'n_estimators': 400, 'reg_alpha': 1.2, 'reg_lambda': 1.3, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGBM\n",
    "The next task was LightGBM for classifying breast cancer. The metric chosen was accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'num_leaves': [50, 100, 200],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'min_split_gain': [0.3, 0.4],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'subsample_freq': [20]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters and best score from the GridSearchCV on the breast cancer dataset with LightGBM was\n",
    "\n",
    ">0.9736263736263736\n",
    "{'colsample_bytree': 0.7, 'max_depth': 15, 'min_split_gain': 0.3, 'n_estimators': 400, 'num_leaves': 50, 'reg_alpha': 1.3, 'reg_lambda': 1.1, 'subsample': 0.7, 'subsample_freq': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sklearn\n",
    "Just to show that you indeed can run GridSearchCV with one of sklearn's own estimators, I tried the RandomForestClassifier on the same dataset as LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'max_depth': [15,20,25],\n",
    "    'max_leaf_nodes': [50, 100, 200]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed the score was worse than from LightGBM, as expected:\n",
    "\n",
    ">0.9648351648351648\n",
    "{'max_depth': 25, 'max_leaf_nodes': 50, 'n_estimators': 1000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV\n",
    "\n",
    "We don't have to restrict ourselves to GridSearchCV â€“ why not implement RandomSearchCV too, if that is preferable to you. This is implemented at the bottom of the notebook available here.\n",
    "\n",
    "We can specify another parameter for the pipeline search_mode, which let's us specify which search algorithm we want to use in our pipeline. But we also introduce another parameter called n_iterations, since we need to provide such a parameter for both the RandomSearchCV class â€“ but not GridSearchCV.\n",
    "\n",
    "We can set the default for both those parameters, and indeed that is what I have done. search_mode = 'GridSearchCV' and n_iterations = 0 is the defaults, hence we default to GridSearchCV where the number of iterations is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def search_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False, search_mode = 'GridSearchCV', n_iterations = 0):\n",
    "    fitted_model = None\n",
    "    \n",
    "    if(search_mode == 'GridSearchCV'):\n",
    "        gs = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid, \n",
    "            cv=cv, \n",
    "            n_jobs=-1, \n",
    "            scoring=scoring_fit,\n",
    "            verbose=2\n",
    "        )\n",
    "        fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "\n",
    "    elif (search_mode == 'RandomizedSearchCV'):\n",
    "        rs = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid, \n",
    "            cv=cv,\n",
    "            n_iter=n_iterations,\n",
    "            n_jobs=-1, \n",
    "            scoring=scoring_fit,\n",
    "            verbose=2\n",
    "        )\n",
    "        fitted_model = rs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    \n",
    "    if(fitted_model != None):\n",
    "        if do_probabilities:\n",
    "            pred = fitted_model.predict_proba(X_test_data)\n",
    "        else:\n",
    "            pred = fitted_model.predict(X_test_data)\n",
    "            \n",
    "        return fitted_model, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this for the breast cancer dataset, it produces the below results, which is almost the same as the GridSearchCV result (which got a score of 0.9648)\n",
    "\n",
    ">0.9626373626373627\n",
    "{'n_estimators': 1000, 'max_leaf_nodes': 100, 'max_depth': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
