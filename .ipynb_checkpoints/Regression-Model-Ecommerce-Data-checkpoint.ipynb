{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a regression model for an e-commerce dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "import seaborn as sns\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "import warnings\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from sklearn.preprocessing import scale\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Ecom_Customers.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kor = df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df_kor, vmin=-1, vmax=1, cmap=\"viridis\", annot=True, linewidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "# To fill, you can use fillna():\n",
    "\n",
    "df[\"Time on App\"].fillna(df[\"Time on App\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df[\"Yearly Amount Spent\"]\n",
    "\n",
    "X=df[[ \"Length of Membership\", \"Time on App\", \"Time on Website\", 'Avg. Session Length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print('Training Data Count: {}'.format(X_train.shape[0]))\n",
    "print('Testing Data Count: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "results = sm.OLS(y_train, X_train).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the outputs of the model: Is this statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what do all those numbers mean actually?**\n",
    "\n",
    "Before continuing, it will be better to explain these basic statistical terms here because I will decide if my model is sufficient or not by looking at those numbers.\n",
    "\n",
    "**What is the p-value?**\n",
    "> P-value or probability value shows statistical significance. Let’s say you have a hypothesis that the average CTR of your brand keywords is 70% or more and its p-value is 0.02. This means there is a 2% probability that you would see CTRs of your brand keywords below %70. Is it statistically significant? 0.05 is generally used for max limit (95% confidence level), so if you have p-value smaller than 0.05, yes! It is significant. The smaller the p-value is, the better your results!\n",
    "\n",
    "Now let’s look at the summary table. My 4 variables have some p-values showing their relations whether significant or insignificant with Yearly Amount Spent. As you can see, Time on Website is statistically insignificant with it because its p-value is 0.180. So it will be better to drop it.\n",
    "\n",
    "**What is R squared and Adjusted R squared?**\n",
    "> R square is a simple but powerful metric that shows how much variance is explained by the model. It counts all variables you defined in X and gives a percentage of explanation. It is something like your model capabilities. \n",
    "\n",
    "**Adjusted R squared** is also similar to R squared but it counts only statistically significant variables. That is why it is better to look at adjusted R squared all the time.\n",
    "\n",
    "In my model, 98.4% of the variance can be explained, which is really high. \n",
    "\n",
    "**What is Coef?**\n",
    "They are coefficients of the variables which give us the equation of the model.\n",
    "\n",
    "So is it over? No! I have Time on Website variable in my model which is statistically insignificant. \n",
    "\n",
    "Now I will build another model and drop Time on Website variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=df[[\"Length of Membership\", \"Time on App\", 'Avg. Session Length']]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print('Training Data Count:', X2_train.shape[0])\n",
    "print('Testing Data Count::', X2_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = sm.add_constant(X2_train)\n",
    "\n",
    "results2 = sm.OLS(y2_train, X2_train).fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared is still good and I have no variable having p-value higher than 0.05.\n",
    "\n",
    "Let’s look at the model chart here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test = sm.add_constant(X2_test)\n",
    "\n",
    "y2_preds = results2.predict(X2_test)\n",
    "\n",
    "plt.figure(dpi = 75)\n",
    "plt.scatter(y2_test, y2_preds)\n",
    "plt.plot(y2_test, y2_test, color=\"red\")\n",
    "plt.xlabel(\"Actual Scores\", fontdict=ex_font)\n",
    "plt.ylabel(\"Estimated Scores\", fontdict=ex_font)\n",
    "plt.title(\"Model: Actual vs Estimated Scores\", fontdict=header_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like I predict values really good! Actual scores and predicted scores have almost perfect linearity.\n",
    "\n",
    "Finally, I will check the errors.\n",
    "\n",
    "**Errors**\n",
    "When building models, comparing them and deciding which one is better is a crucial step. You should test lots of things and then analyze summaries. Drop some variables, sum or multiply them and again test. After completing the series of analysis, you will check p-values, errors and R squared. The best model will have:\n",
    "\n",
    "- P-values smaller than 0.05\n",
    "- Smaller errors\n",
    "- Higher adjusted R squared\n",
    "\n",
    "Let’s look at errors now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error (MAE)         : {}\".format(mean_absolute_error(y2_test, y2_preds)))\n",
    "print(\"Mean Squared Error (MSE) : {}\".format(mse(y2_test, y2_preds)))\n",
    "print(\"Root Mean Squared Error (RMSE) : {}\".format(rmse(y2_test, y2_preds)))\n",
    "print(\"Root Mean Squared Error (RMSE) : {}\".format(rmse(y2_test, y2_preds)))\n",
    "print(\"Mean Absolute Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y2_test - y2_preds) / y2_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know what MSE, RMSE or MAPE is, you can read this article.\n",
    "\n",
    "They are all different calculations of errors and now, we will just focus on smaller ones while comparing different models.\n",
    "\n",
    "So, in order to compare my model with another one, I will create one more model including Length of Membership and Time on App only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=df[['Length of Membership', 'Time on App']]\n",
    "Y = df['Yearly Amount Spent']\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "X3_train = sm.add_constant(X3_train)\n",
    "\n",
    "results3 = sm.OLS(y3_train, X3_train).fit()\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_test = sm.add_constant(X3_test)\n",
    "y3_preds = results3.predict(X3_test)\n",
    "\n",
    "plt.figure(dpi = 75)\n",
    "plt.scatter(y3_test, y3_preds)\n",
    "plt.plot(y3_test, y3_test, color=\"red\")\n",
    "plt.xlabel(\"Actual Scores\", fontdict=eksen_font)\n",
    "plt.ylabel(\"Estimated Scores\", fontdict=eksen_font)\n",
    "plt.title(\"Model Actual Scores vs Estimated Scores\", fontdict=baslik_font)\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Absolute Error (MAE)\n",
    ": {}\".format(mean_absolute_error(y3_test, y3_preds)))\n",
    "print(\"Mean Squared Error (MSE) : {}\".format(mse(y3_test, y3_preds)))\n",
    "print(\"Root Mean Squared Error (RMSE) :\n",
    "{}\".format(rmse(y3_test, y3_preds))) print(\"Mean Absolute Perc. Error (MAPE) :\n",
    "{}\".format(np.mean(np.abs((y3_test - y3_preds) / y3_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "data = pd.read_csv('indian_liver_patient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_use = data\n",
    "del data_to_use['Gender']\n",
    "data_to_use.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data_to_use.values\n",
    "\n",
    "Y = values[:,9]\n",
    "X = values[:,0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "\n",
    "outcome = []\n",
    "model_names = []\n",
    "models = [('LogReg', LogisticRegression()), \n",
    "          ('SVM', SVC()), \n",
    "          ('DecTree', DecisionTreeClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('LinDisc', LinearDiscriminantAnalysis()),\n",
    "          ('GaussianNB', GaussianNB())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a k-fold validation to evaluate each algorithm and will run through each model with a for loop, running the analysis and then storing the outcomes into the lists we created above. We’ll use a 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models:\n",
    "    k_fold_validation = model_selection.KFold(n_splits=10, random_state=random_seed)\n",
    "    results = model_selection.cross_val_score(model, X, Y, cv=k_fold_validation, scoring='accuracy')\n",
    "    outcome.append(results)\n",
    "    model_names.append(model_name)\n",
    "    output_message = \"%s| Mean=%f STD=%f\" % (model_name, results.mean(), results.std())\n",
    "    print(output_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output\n",
    "- LogReg| Mean=0.718633 STD=0.058744\n",
    "- SVM| Mean=0.715124 STD=0.058962\n",
    "- DecTree| Mean=0.637568 STD=0.108805\n",
    "- KNN| Mean=0.651301 STD=0.079872\n",
    "- LinDisc| Mean=0.716878 STD=0.050734\n",
    "- GaussianNB| Mean=0.554719 STD=0.081961"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it looks like the Logistic Regression, Support Vector Machine and Linear Discrimination Analysis methods are providing the best results (based on the ‘mean’ values). Taking Jason’s lead, we can take a look at a box plot to see what the accuracy is for each cross validation fold, we can see just how good each does relative to each other and their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Machine Learning Model Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(outcome)\n",
    "ax.set_xticklabels(model_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plot, when it is easy to see the three mentioned machine learning methods (Logistic Regression, Support Vector Machine and Linear Discrimination Analysis) are providing better accuracies. From this outcome, we can then take this data and start working with these three models to see how we might be able to optimize the modeling process to see if one model works a bit better than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
