{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "X_train_robust = RobustScaler().fit(X)\n",
    "\n",
    "X_train_robust.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Looking For Most Relevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "# Features are in train and labels are in train_labels\n",
    "fs = FeatureSelector(data = train, labels = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X.shape\n",
    "\n",
    "X_new = SelectKBest(chi2, k=5).fit_transform(X_train_minmax, train_y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = diabetes.feature_names\n",
    "\n",
    "feature_names = X_columns\n",
    "#print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LassoCV().fit(X, y)\n",
    "importance = np.abs(clf.coef_)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_third = importance.argsort()[-3]\n",
    "threshold = importance[idx_third] + 0.01\n",
    "\n",
    "idx_features = (-importance).argsort()[:4]\n",
    "name_features = np.array(feature_names)[idx_features]\n",
    "print('Selected features: {}'.format(name_features))\n",
    "\n",
    "sfm = SelectFromModel(clf, threshold=threshold)\n",
    "sfm.fit(X, y)\n",
    "X_transform = sfm.transform(X)\n",
    "\n",
    "n_features = sfm.transform(X).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's predict for test data and generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['Prediction'] = np.expm1(best_xgb_model.predict(test_dataset[most_relevant_features]))\n",
    "filename = 'submission.csv'\n",
    "pd.DataFrame({'Id': test_dataset.Id, 'SalePrice': test_dataset.Prediction}).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset['Prediction'].head())\n",
    "print(test_dataset['Prediction'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Loading some example data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Imputation Using MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fancyimpute package takes a single combined matrix as input\n",
    "f = np.hstack((X1, y1[:, None]))\n",
    "\n",
    "print (\"Output stacked array :\\n \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancyimpute import IterativeImputer\n",
    "MICE_imputer = IterativeImputer()\n",
    "f_complete = f.copy()\n",
    "f_complete[:, :] = MICE_imputer.fit_transform(f_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f_complete == np.nan).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y1 == np.nan).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_complete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling in Missing Data Segment by Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Forward and Back Fill Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr_imputed = df_train1.fillna(df_train1.groupby(['galaxy'], as_index=False)\n",
    "                                .fillna(method='ffill')\n",
    "                                .fillna(method='bfill'))\n",
    "dftr_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data By Galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And1 =dftr_imputed[dftr_imputed[\"galaxy\"] == 'Andromeda II']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns2 = dftr_imputed.columns.tolist()[0:79]\n",
    "y_columns2 = dftr_imputed.columns.tolist()[-4:-3]\n",
    "#print(f'All columns: {dftr_imputed.columns.tolist()}')\n",
    "print()\n",
    "print(f'X values: {X_columns2}')\n",
    "print()\n",
    "print(f'y values: {y_columns2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = dftr_imputed[X_columns2].values \n",
    "y2 = dftr_imputed[y_columns2].values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical (string based) data. Country: there are 3 options: France, Spain and Germany\n",
    "# This will convert those strings into scalar values for analysis\n",
    "print(X2[:8,1], '... will now become: ')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_X2_galactic_encoder = LabelEncoder()\n",
    "X2[:,1] = label_X2_galactic_encoder.fit_transform(X2[:,1])\n",
    "print(X2[:8,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X2, test_X2, train_y2, test_y2 = train_test_split(X2, y2, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.fit(train_X2, train_y2)\n",
    "\n",
    "train_pred_y2 = model2.predict(train_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Vs Actual\n",
    "test_pred_y2 = model2.predict(test_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train2 = np.sqrt(mean_squared_error(train_pred_y2, train_y2))\n",
    "\n",
    "# msle_train = mean_squared_log_error(train_pred_y, train_y)\n",
    "rmse_test2 = np.sqrt(mean_squared_error(test_pred_y2, test_y2))\n",
    "\n",
    "# msle_test = mean_squared_log_error(test_pred_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('rmse_train:',rmse_train,'msle_train:',msle_train)\n",
    "# print('rmse_test:',rmse_test,'msle_test:',msle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for rmse, the lower that value is, the better the fit\n",
    "print('rmse_train:',rmse_train2)\n",
    "print('rmse_test:',rmse_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The closer towards 1, the better the fit\n",
    "test_set_r2_2 = r2_score(test_y2, test_pred_y2)\n",
    "test_set_r2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output modified dataset to CSV\n",
    "df.to_csv('mock_bank_data_original_PART2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = f_complete[:, 0:79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1, test_X1, train_y1, test_y1 = train_test_split(f_train, y1, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression()\n",
    "model1.fit(train_X1, train_y1)\n",
    "\n",
    "train_pred_y1 = model1.predict(train_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Vs Actual\n",
    "test_pred_y1 = model1.predict(test_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train1 = np.sqrt(mean_squared_error(train_pred_y1, train_y1))\n",
    "\n",
    "# msle_train1 = mean_squared_log_error(train_pred_y1, train_y1)\n",
    "rmse_test1 = np.sqrt(mean_squared_error(test_pred_y1, test_y1))\n",
    "\n",
    "# msle_test1 = mean_squared_log_error(test_pred_y1, test_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('rmse_train:',rmse_train,'msle_train:',msle_train)\n",
    "# print('rmse_test:',rmse_test,'msle_test:',msle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for rmse, the lower that value is, the better the fit\n",
    "print('rmse_train:',rmse_train1)\n",
    "print('rmse_test:',rmse_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The closer towards 1, the better the fit\n",
    "test_set_r3 = r2_score(test_y1, test_pred_y1)\n",
    "test_set_r3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit linear Model for Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "diabetes_cc = diabetes.dropna(how=\n",
    "'any')\n",
    "X = sm.add_constant(diabetes_cc.iloc[:, :-1])\n",
    "y = diabetes_cc['Class']\n",
    "lm = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-squared and Coefcients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "# Create linear regression object\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train) #training the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting The Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To retrieve the intercept:\n",
    "print(regressor.intercept_)\n",
    "#For retrieving the slope:\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Vs Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  \n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=13)\n",
    "\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Categorical Features For Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "#categorical_features = train.select_dtypes(include=['object']).drop(['y'], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "#categorical_features = train.select_dtypes(include=['object']).drop(['y'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Transformer Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Both Numeric & Categorical\n",
    "\n",
    "I then use the ColumnTransfomer to concatenate both the numeric and categorical transformers into an object called preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.compose import ColumnTransformer\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numeric_transformer, numeric_features),\n",
    "#        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "lr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_train.groupby([\"galaxy\", \"galactic year\"], as_index=False)[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp.fit(train_X1)\n",
    "\n",
    "test_X1 = np.round(imp.transform(test_X1))\n",
    "test_X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_y1 = reg_b.predict(train_X)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(train_y, train_pred_y1)))\n",
    "print(r2_score(train_y, train_pred_y1))\n",
    "print()\n",
    "test_pred_y1 = reg_b.predict(test_X)\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('R2 score: {:.2f}'.format(r2_score(test_y, test_pred_y1)))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_y,test_pred_y1))) \n",
    "print(r2_score(test_y,test_pred_y1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_b.transform(test_X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train1 = np.sqrt(mean_squared_error(train_pred_y1, train_y))\n",
    "\n",
    "# msle_train = mean_squared_log_error(train_pred_y, train_y)\n",
    "rmse_test1 = np.sqrt(mean_squared_error(test_pred_y1, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for rmse, the lower that value is, the better the fit\n",
    "print('rmse_train1:',rmse_train1)\n",
    "print('rmse_test1:',rmse_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The closer towards 1, the better the fit\n",
    "test_set_r2_1 = r2_score(test_y, test_pred_y1)\n",
    "test_set_r2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Manipulate Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = sum(importance) \n",
    "  \n",
    "# display sum \n",
    "print ('Sum of the array is ',ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "  \n",
    "a = np.mod(importance, 4) !=0\n",
    "# This will show element status of satisfying condition \n",
    "print(\"\\nArray Condition a : \\n\", a) \n",
    "  \n",
    "# This will return elements that satisy condition \"a\" condition \n",
    "print(\"\\nMost Important Features  : \\n\", np.extract(a, array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Important Features  : \n",
      "\n",
      "0.32195789894605387\n",
      "0.22511424313468498\n",
      "0.1021760748583806\n",
      "0.04929789284170807\n",
      "0.04758076591727947\n",
      "0.03414135182997575\n",
      "0.030069040425309616\n",
      "0.027515871164306264\n",
      "0.021141095936650568\n",
      "0.013725055712194041\n",
      "0.01030059958646799\n",
      "0.010047663765946112\n",
      "0.00927636464364994\n",
      "0.008844727470055378\n",
      "0.0085120110167444\n",
      "0.003898332232637567\n",
      "0.0035902334994072833\n",
      "0.0032819779974912326\n",
      "0.003267353576194024\n",
      "0.003230056884630819\n",
      "0.0029551444489928167\n",
      "0.002711749936359411\n",
      "0.002692978772717507\n",
      "0.0023033546951820565\n",
      "0.0022520249250871822\n",
      "0.0020209458744506327\n"
     ]
    }
   ],
   "source": [
    "b = [x for x in importance if x > 0.00202]\n",
    "\n",
    "print(\"\\nMost Important Features  : \\n\")\n",
    "for x in b:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['new','color'])\n",
    "df.set_index('new').sort_index(ascending=False)\n",
    "df.sort_values(['Rev', 'Rev2'], ascending=[False,False])\n",
    "df.sort_values(['Rev', 'Rev2'], ascending=[False,True])\n",
    "\n",
    "df.set_index('name', inplace=True)\n",
    "print(df)\n",
    "\n",
    "# sorting data frame by name \n",
    "data.sort_values(\"Name\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') \n",
    "  \n",
    "# display \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Duplicates Rows Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def main():\n",
    "    # List of Tuples\n",
    "    students = [('jack', 34, 'Sydeny'),\n",
    "                ('Riti', 30, 'Delhi'),\n",
    "                ('Aadi', 16, 'New York'),\n",
    "                ('Riti', 30, 'Delhi'),\n",
    "                ('Riti', 30, 'Delhi'),\n",
    "                ('Riti', 30, 'Mumbai'),\n",
    "                ('Aadi', 40, 'London'),\n",
    "                ('Sachin', 30, 'Delhi')\n",
    "                ]\n",
    "    # Create a DataFrame object\n",
    "    dfObj = pd.DataFrame(students, columns=['Name', 'Age', 'City'])\n",
    "    print(\"Original Dataframe\", dfObj, sep='\\n')\n",
    "    print('*** Find Duplicate Rows based on all columns ***')\n",
    "    # Select duplicate rows except first occurrence based on all columns\n",
    "    duplicateRowsDF = dfObj[dfObj.duplicated()]\n",
    "    print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "    print(duplicateRowsDF)\n",
    "    # Select duplicate rows except last occurrence based on all columns\n",
    "    duplicateRowsDF = dfObj[dfObj.duplicated(keep='last')]\n",
    "    print(\"Duplicate Rows except last occurrence based on all columns are :\")\n",
    "    print(duplicateRowsDF)\n",
    "    # Select all duplicate rows based on all columns\n",
    "    duplicateRowsDF = dfObj[dfObj.duplicated(keep=False)]\n",
    "    print(\"All Duplicate Rows based on all columns are :\")\n",
    "    print(duplicateRowsDF)\n",
    "    # Select all duplicate rows based on one column\n",
    "    duplicateRowsDF = dfObj[dfObj.duplicated(['Name'])]\n",
    "    print(\"Duplicate Rows based on a single column are:\", duplicateRowsDF, sep='\\n')\n",
    "    # Select all duplicate rows based on multiple column names in list\n",
    "    duplicateRowsDF = dfObj[dfObj.duplicated(['Age', 'City'])]\n",
    "    print(\"Duplicate Rows based on 2 columns are:\", duplicateRowsDF, sep='\\n')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data & Re-Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_test2.set_index(['galaxy', 'GYear'])\n",
    "new_df_train2.set_index(['galaxy', 'GYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data frame by name \n",
    "new_df_train2.sort_values(\"galaxy\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') \n",
    "  \n",
    "# display \n",
    "new_df_train2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data frame by name \n",
    "new_df_test2.sort_values(\"galaxy\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') \n",
    "  \n",
    "# display \n",
    "new_df_test2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Identification & Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_test2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.scatter(df['Income'], df['Loan_amount'])\n",
    "ax.set_xlabel('Income of applicants in USD')\n",
    "ax.set_ylabel('Loan amount applied for in USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Outliers with Skewness\n",
    "\n",
    "Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Income'].skew())\n",
    "df['Income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment\n",
    "\n",
    "#### Quantile-based Flooring and Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Income'].quantile(0.10))\n",
    "print(df['Income'].quantile(0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Income\"] = np.where(df[\"Income\"] <2960.0, 2960.0,df['Income'])\n",
    "df[\"Income\"] = np.where(df[\"Income\"] >12681.0, 12681.0,df['Income'])\n",
    "print(df['Income'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df[(df['Age'] >= 100)|(df['Age'] <= 18)].index\n",
    "df.drop(index, inplace=True)\n",
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = new_df_test2[~((new_df_test2 < (Q1 - 1.5 * IQR)) |(new_df_test2 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(df_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = new_df_train2[~((new_df_train2 < (Q1 - 1.5 * IQR)) |(new_df_train2 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(df_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Log_Loanamt\"] = df[\"Loan_amount\"].map(lambda i: np.log(i) if i > 0 else 0) \n",
    "print(df['Loan_amount'].skew())\n",
    "print(df['Log_Loanamt'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing Outliers with Median Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Values to be used to replace outliers\n",
    "print(df['Loan_amount'].quantile(0.50)) \n",
    "print(df['Loan_amount'].quantile(0.95)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All values above 95% Quantile replaced with mean\n",
    "df['Loan_amount'] = np.where(df['Loan_amount'] > 325, 140, df['Loan_amount'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "# savetxt('oof_preds.csv', data, delimiter=',')\n",
    "\n",
    "# load numpy array from csv file\n",
    "from numpy import loadtxt\n",
    "# load array\n",
    "data = loadtxt('data.csv', delimiter=',')\n",
    "# print the array\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.array([[1, 2], [3, 4]])\n",
    "df = pd.DataFrame(data=numpy_data, index=[\"row1\", \"row2\"], columns=[\"column1\", \"column2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv without index or headers\n",
    "df.to_csv('file.csv', index=False, header=False)\n",
    "\n",
    "print(df.to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1['ID'] = range(1, len(df_train1.index)+1)\n",
    "df_test1['ID'] = range(1, len(df_test1.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting & Re-Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_test2.set_index(['galaxy', 'GYear'])\n",
    "new_df_train2.set_index(['galaxy', 'GYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data frame by name \n",
    "new_df_train2.sort_values(\"galaxy\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') \n",
    "  \n",
    "# display \n",
    "new_df_train2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data frame by name \n",
    "new_df_test2.sort_values(\"galaxy\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') \n",
    "  \n",
    "# display \n",
    "new_df_test2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(df_sel_data)\n",
    "X_Trans = scaler.transform(df_sel_data)\n",
    "#scaler.transform(df_sel_target)\n",
    "\n",
    "#X_Trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scale data matrix to the [0, 1] range:\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "data = QuantileTransformer(output_distribution='normal')\n",
    "        .fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "\n",
    "# Take only 2 features to make visualization easier\n",
    "# Feature of 0 has a long tail distribution.\n",
    "# Feature 5 has a few but very large outliers.\n",
    "\n",
    "X = X_full[:, [0, 5]]\n",
    "\n",
    "distributions = [\n",
    "    ('Unscaled data', X),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        .fit_transform(X)),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        .fit_transform(X)),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer().fit_transform(X)),\n",
    "]\n",
    "\n",
    "# scale the output between 0 and 1 for the colorbar\n",
    "y = minmax_scale(y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
